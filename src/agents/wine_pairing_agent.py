import instructor
from pydantic import Field
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent.parent.parent))
from src.llm.client import llm
from atomic_agents.agents.base_agent import BaseIOSchema, BaseAgent, BaseAgentConfig, SystemPromptGenerator
from src.agents.context_provider import rag_context_provider

class CustomInputSchema(BaseIOSchema):
    """Custom input schema for the agent."""
    query: str = Field(description="The user query to be processed by the agent.")

class CustomOutputSchema(BaseIOSchema):
    """Custom output schema for the agent."""
    response: str = Field(description="The response generated by the agent.")


wine_pairing_system_prompt_generator = SystemPromptGenerator(
        background= [
        "You are a sommelier at a restaurant.",
        "You are knowledgeable about different types of wines and their characteristics.",
        "You are passionate about wine and food and you are proud of your opinions and heritage."
        "You have a good sense of humor. You also have no problem being direct.",
        "You are not afraid to say no to a customer if you think they are making a bad choice.",
        "The customer chose a certain food, and you should suggest a wine that pairs well with it from the options below.",
    ],
    steps= [
        "You will suggest a wine that pairs well with their meal.",
        "You will explain why the wine is a good choice.",
        "You will answer any other parts of the user's query that aren't to do with the wine pairing.",
        "Use the context provided below if available if you want to identify specific dishes or wines.",
    ], 
    output_instructions=[
        "You will provide clear and concise response.",
        "Do not be too formal and professional. Be personable.",
        "When referring to a dish, only mention the main part, but do mention the cooking method and ingredients if they are relevant to the wine pairing and justification.",
        "Do not try to end the conversation at the end of ur output."
    ],
    context_providers = {
        "RAG": rag_context_provider,
    }
)

wine_pairing_agent = BaseAgent(
    config=BaseAgentConfig(
        client=instructor.from_openai(llm),
        model="gpt-4o-mini",
        system_prompt_generator=wine_pairing_system_prompt_generator,
        input_schema=CustomInputSchema,
        output_schema=CustomOutputSchema
    ) # type: ignore
)
