import instructor
from rich.console import Console
from pydantic import BaseModel, Field
import logfire
from typing import List, TypedDict
import sys
from pathlib import Path
# from atomic.tools

sys.path.append(str(Path(__file__).parent.parent.parent))
from experimenting.query_chroma import get_retriever
from src.llm.client import get_llm
from atomic_agents.agents.base_agent import BaseIOSchema, BaseAgent, BaseAgentConfig, BaseAgentInputSchema, AgentMemory, SystemPromptGenerator, SystemPromptContextProviderBase

console = Console()
memory = AgentMemory()


# class ChunkItem(BaseModel): #TODO: Find out structure of retriever.retieve() and apply to this
#     content: str
#     metadata: dict


class RAGContextProvider(SystemPromptContextProviderBase):
    """Context provider for RAG (Retrieval-Augmented Generation)."""
    def __init__(self, title: str):
        super().__init__(title)
        self.chunks = []

    def set_chunks(self, chunks): #TODO: set chunks: List[ChunkItem])
        self.chunks = chunks

    def get_info(self) -> str:
        if not self.chunks:
            return "No relevant information found."
        context_info = ""
        for node in self.chunks:
            context_info += f"{node.get_content()}\n"
        return context_info
    


class CustomInputSchema(BaseIOSchema):
    """Custom input schema for the agent."""
    query: str = Field(description="The query to be processed by the agent.")
    # context: str = Field(description="The context in which the query is made.")

class CustomOutputSchema(BaseIOSchema):
    """Custom output schema for the agent."""
    response: str = Field(description="The response generated by the agent.")
    # context: str = Field(description="The context in which the query was made.")

initial_message = CustomOutputSchema(
    response="Hello! Welcome to our restaurant.\
        Can I interest you in any wine to pair with your meal?",
#     context="You are a sommelier at a restaurant.\
#         You are here to help customers choose the best wine to pair with their meal.")
)

memory.add_message("assistant", initial_message)
rag_context_provider = RAGContextProvider(title="Food & Wine Menu RAG Retrieval")


system_prompt_generator = SystemPromptGenerator(
    background= [
        "You are a sommelier at a restaurant.",
        "You are here to help customers choose the best wine to pair with their meal.",
        "You are knowledgeable about different types of wines and their characteristics.",
        "You are passionate about wine and food and you are proud of your opinions and heritage."
        "You are friendly and personable. You have a good sense of humor. You also have no problem being direct.",
        "You are not afraid to say no to a customer if you think they are making a bad choice."
    ],
    steps= [
        "You will ask the customer about their meal and preferences.",
        "You will suggest a wine that pairs well with their meal.",
        "You will explain why the wine is a good choice.",
        "If the user gauges interest, you will provide additional information about the wine, such as its region and flavor profile."
    ], 
    output_instructions=[
        "You will provide clear and concise response.",
        "You will friendly, but show passion in your responses, engaging the user.",
        "Do not be too formal and professional. Be personable.",
        "When identifying a dish, only mention the main ingredient. Do mention the cooking method and ingredients if they are relevant to the wine pairing.",
    ],
    context_providers = {
        "RAG": rag_context_provider,
    }

)

agent = BaseAgent(
    config=BaseAgentConfig(
        client=instructor.from_openai(get_llm()),
        model="gpt-4o-mini",
        system_prompt_generator=system_prompt_generator,
        memory=memory,
        input_schema=CustomInputSchema,
        output_schema=CustomOutputSchema
    ) # type: ignore 
)

while True:
    # Prompt the user for input
    user_input = console.input("[bold blue]You:[/bold blue] ")
    # Check if the user wants to exit the chat
    if user_input.lower() in ["/exit", "/quit"]:
        console.print("Exiting chat...")
        break

    # Query the RAG with the user's input
    retriever = get_retriever() # -> BaseRetriever
    context = retriever.retrieve(user_input) # -> List[NodeWithScore]

    if context:
        rag_context_provider.set_chunks(context)

    # Process the user's input through the agent
    input_schema = BaseAgentInputSchema(chat_message=user_input)
    response = agent.run(input_schema)

    # Display the agent's response
    console.print("[bold green]Agent:[/bold green] ", response)
